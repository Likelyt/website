<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title></title>
    <link>https://liyuantong93.com/</link>
    <description>Recent content on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018 Yuantong Li. All rights reserved.</copyright>
    <lastBuildDate>Sun, 21 Jul 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://liyuantong93.com/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Moreau-Yosida regularization</title>
      <link>https://liyuantong93.com/2019/07/21/</link>
      <pubDate>Sun, 21 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://liyuantong93.com/2019/07/21/</guid>
      <description>UCLA ECE236C: Optimization Methods for Large-scale Systems Notes</description>
    </item>
    
    <item>
      <title>Conjugated gradient</title>
      <link>https://liyuantong93.com/2019/07/20/</link>
      <pubDate>Sat, 20 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://liyuantong93.com/2019/07/20/</guid>
      <description>Two vectors \(u\) and \(v\) are conjugate with respect to the matrix \(A\) if \(u^{\prime} A v = 0\).
Suppose we want to minimize the quadratic function \[ f(x)=\frac{1}{2} x^{\prime} A x-x^{\prime} b \] where \(x\) is a \(p\)-dimensional vector and \(A\) is a \(p\times p\) symmetric matrix. \(p_{0}=-f^{\prime}\left(x_{0}\right)=b-A x_{0}\), which is the negative gradient. So we can begin with the steepest descent direction \(-f^{\prime}\left(x_{1}\right)\) but then we must modify it to make it conjugate to \(p_{0}\).</description>
    </item>
    
    <item>
      <title>Generalized Linear Model</title>
      <link>https://liyuantong93.com/2019/07/18/</link>
      <pubDate>Thu, 18 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://liyuantong93.com/2019/07/18/</guid>
      <description>In statistics, the generalized linear model (GLM) is a flexible generalization of ordinary linear regression that allows for response variables that have error distribution models other than a normal distribution. The GLM generalizes linear regression by allowing the linear model to be related to the response variable via a link function and by allowing the magnitude of the variance of each measurement to be a function of its predicted value.</description>
    </item>
    
    <item>
      <title>Stochastic Variational Inference</title>
      <link>https://liyuantong93.com/2019/07/17/</link>
      <pubDate>Wed, 17 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://liyuantong93.com/2019/07/17/</guid>
      <description>Some slides Nips 2016, CMU Lecture 12, CMU Lecture 13</description>
    </item>
    
    <item>
      <title>Factor Analysis and PCA</title>
      <link>https://liyuantong93.com/2019/07/14/</link>
      <pubDate>Sun, 14 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://liyuantong93.com/2019/07/14/</guid>
      <description>The Factor Analysis note from CMU.
The PCA</description>
    </item>
    
    <item>
      <title>Graph Convolutional Networks</title>
      <link>https://liyuantong93.com/2019/07/12/</link>
      <pubDate>Fri, 12 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://liyuantong93.com/2019/07/12/</guid>
      <description>Two types of GCN:
Defferrard, M., Bresson, X., &amp;amp; Vandergheynst, P. (2016). Paper
 Kipf, T. N., &amp;amp; Welling, M. (2016). Paper
  </description>
    </item>
    
    <item>
      <title>Statistical Learning</title>
      <link>https://liyuantong93.com/2019/07/12/</link>
      <pubDate>Fri, 12 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://liyuantong93.com/2019/07/12/</guid>
      <description>Details can be viewed in my Github Page</description>
    </item>
    
    <item>
      <title>Information Retrieval</title>
      <link>https://liyuantong93.com/2019/05/28/</link>
      <pubDate>Tue, 28 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://liyuantong93.com/2019/05/28/</guid>
      <description>In IR, the true loss functions can be those defined based on NDCG (Normalized Discounted Cumulative Gain) and MAP (Mean Average Precision).</description>
    </item>
    
    <item>
      <title>Sampling Methods and Monte Carlo methods</title>
      <link>https://liyuantong93.com/2018/08/22/</link>
      <pubDate>Wed, 22 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://liyuantong93.com/2018/08/22/</guid>
      <description>Suppose we have a function \(f(x)\) and we’d like to compute \(E{f(X )} =\int f(x)p(x)dx\), where \(g(x)\) is a density.
There is no guarantee that the techniques we learn in calculus are sufficient to evaluate this integral analytically.
Thankfully, the law of large numbers (LLN) is here to help:
If X1, X2, . . . are iid samples from \(g(x)\), then
\(\frac{1}{n}\sum^{n}_{i=1}f(x_{i}) \rightarrow \int f(x)p(x)dx = E[f(x)]\) with prob 1</description>
    </item>
    
    <item>
      <title>EM Algorithms</title>
      <link>https://liyuantong93.com/2018/08/20/</link>
      <pubDate>Mon, 20 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://liyuantong93.com/2018/08/20/</guid>
      <description>Original Paper: EM algorithm
Lecture Notes 1, 2, 3
Wikipedia gives a really great explanantion 3
UCI CS note
Nine step understand EM Whole map</description>
    </item>
    
    <item>
      <title>Create Multiple Directories and Files With Contents</title>
      <link>https://liyuantong93.com/2018/08/06/</link>
      <pubDate>Mon, 06 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://liyuantong93.com/2018/08/06/</guid>
      <description>1. Create multiple directories for i in {1..6}; do for j in {1..5}; do for k in {1..10}; do for z in 0.00100 0.00200 0.00300 0.00400 0.00500 0.00600 0.00700 0.00800; do mkdir -p network_$i/CV_$j/layer_$k/lr_$z done done done done or
mkdir -p network_1/{CV_1,CV_2,CV_3,done}  2. Create multiple files Step 1. Create A Dir mkdir file  Step 2. Move to Dir cd file/  Step 3. Create 100 python file touch Name{0001.</description>
    </item>
    
    <item>
      <title>Linear Time Series</title>
      <link>https://liyuantong93.com/2018/08/01/</link>
      <pubDate>Wed, 01 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://liyuantong93.com/2018/08/01/</guid>
      <description>1. Introduction to Time Series Random Walk with drift:
\[X_t = \delta + X_{t-1} + W_{t}\]
library(astsa, quietly = TRUE) x = ts(cumsum(rnorm(500) + 0.2)) plot(x) Autocovariance function:
\[\gamma_{X}(s,t) = E[(X_s - \mu_{X,s})(X_t - \mu_{X,t})]\] and we know \(\gamma_{X}(s,t) = \gamma_{X}(t,s)\)
Autocorrelation function (ACF):
\[\rho_{X}(s,t) = \frac{\gamma_{X}(s,t)}{\sqrt{\gamma_{X}(s,s)}\sqrt{\gamma_{X}(t,t)}}\]
Cross correlation function (CCF):
\[\rho_{X,Y}(s,t) = \frac{\gamma_{X,Y}(s,t)}{\sqrt{\gamma_{X}(s,s)}\sqrt{\gamma_{Y}(t,t)}}\]
Strong (or strict) stationarity  A time series \(X_t\) is called strongly stationary if the joint distribution of every finite collection of variables remain the same under time shifts, i.</description>
    </item>
    
    <item>
      <title>Monte Carlo In Financial Derivatives</title>
      <link>https://liyuantong93.com/2018/07/31/</link>
      <pubDate>Tue, 31 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://liyuantong93.com/2018/07/31/</guid>
      <description>1. How to estimate Pi with Python #library(reticulate) from random import random from math import pow, sqrt simulations = 5000 hits = 0.0 for x in xrange(simulations): x = random() #0 &amp;lt;= x &amp;lt;= 1 y = random() #distance to (0,0) dist = sqrt(pow(x,2) + pow(y,2)) if dist &amp;lt;= 1: hits += 1 print(4 * hits/simulations) ## 3.156  2. Pricing Options using Monte Carlo A geometric Brownian motion is a random process where the logarithm of the random variable follows a normal distribution.</description>
    </item>
    
    <item>
      <title>Options</title>
      <link>https://liyuantong93.com/2018/07/29/</link>
      <pubDate>Sun, 29 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://liyuantong93.com/2018/07/29/</guid>
      <description>3. Options 3.1 Option Basic Two Types of Options: Call (right buy) and Put (right sell).
American Options can exercise at any time up to and including expiration.
A. Moneyness A call option is in-the-money if the asset price is greater than the exercise price (i.e., if it would be worth something exercised).
 A put option is in-the-money if the asset price is less than the exercise price (i.e., if it would be worth something exercised).</description>
    </item>
    
    <item>
      <title>Derivatives</title>
      <link>https://liyuantong93.com/2018/07/23/</link>
      <pubDate>Mon, 23 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://liyuantong93.com/2018/07/23/</guid>
      <description>1. Forward  2. Future  4. VaR Model  5. Additional 5.1 Risk-Adjusted Return 5.1.1 Sharpe ratio \[\text{sharpe ratio} = \frac{\text{excess returns}}{\text{volatility}} = \frac{\mu - r}{\sigma}\]
 5.1.2 Treynor Ratio \[\frac{\mu - r}{\beta}\]
 5.1.3 Information Ratio  5.1.4 Jensen Alpha  5.1.5 M^2    6. Hedge Fund Strategy LP, GP: management fee and incentive fee, usually 2&amp;amp;20 (hurdle rate).
Similarity and difference between Hedge Fund &amp;amp; Mutual Fund</description>
    </item>
    
    <item>
      <title>VIM Basic Command</title>
      <link>https://liyuantong93.com/2018/07/20/</link>
      <pubDate>Fri, 20 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://liyuantong93.com/2018/07/20/</guid>
      <description>Enjoy Vim! Vim Cheat Sheet
Download website: Vim Download</description>
    </item>
    
    <item>
      <title>Welcome!</title>
      <link>https://liyuantong93.com/2018/07/18/</link>
      <pubDate>Wed, 18 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://liyuantong93.com/2018/07/18/</guid>
      <description>Welcome to Yuantong’s website! This is the first post of this website.</description>
    </item>
    
    <item>
      <title>Home</title>
      <link>https://liyuantong93.com/home/</link>
      <pubDate>Mon, 02 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://liyuantong93.com/home/</guid>
      <description>Welcome to Yuantong Li&amp;rsquo;s website! (@_@) I&amp;rsquo;m Yuantong Li, graduated from North Carolina State University with M.S. degree in Statistics in 2018 and will move to Purdue University to pursue a Ph.D. degree in Statistics in this August.
My email address is: liyuantong93@gmail.com
Research Interests:  High-dimensional statistics, graphical model, Bayesian methods
 Deep learning, machine learning
  Education:  Ph.D. in Statistics, Purdue University, 2019 - Now.</description>
    </item>
    
  </channel>
</rss>