---
title: Sampling Methods and Monte Carlo methods
author: Yuantong
date: '2018-08-22'
slug: sampling-method
categories:
  - Bayesian
  - Computing
  - MCMC
tags:
  - Bayesian
  - Computing
  - MCMC
description: ''
topics: []
---

Suppose we have a function $f(x)$ and we’d like to compute $E{f(X )} =\int f(x)p(x)dx$, where $g(x)$ is a density.

There is no guarantee that the techniques we learn in calculus are sufficient to evaluate this integral analytically.

Thankfully, the law of large numbers (LLN) is here to help:

If X1, X2, . . . are iid samples from $g(x)$, then

$\frac{1}{n}\sum^{n}_{i=1}f(x_{i}) \rightarrow \int f(x)p(x)dx = E[f(x)]$ with prob 1

Suggests that a generic approximation of the integral be obtained by sampling lots of Xi ’s from $g(x)$ and replacing integration with averaging.

This is the heart of the Monte Carlo method.

# 1. Naive method
## 1. Inverse of CDF
Inverse transform sampling is a method for generating random numbers from any probability distribution by using its inverse cumulative distribution $F^{-1}(x)$. Recall that the cumulative distribution for a random variable X
is $F_{X}(x)=P(X \leq x)$. A random variable $U$ uniformly distributed on $[0,1]$.

Step 1. Generate $U \sim Unif(0,1)$.

Step 2. Let $X = F^{-1}_{X}(U)$.

This method works when inverting $F_{X}$ is easy if $X$ is an exponential random variable, but its harder if $X$ is a Normal random variable. It works for discrete distributions.

# 2. Monte Carlo method
## 1. Accept - Reject Sampling
The algorithm (used by John von Neumann and dating back to Buffon and his needle) to obtain a sample from distribution $X$ with density $f$ using samples from distribution $Y$ with density $g$ is as follows:

Step 1. Obtain a sample $y$ from distribution $Y$ and a sample$u$ from $Unif(0,1)$.

Step 2. Check whether or not $u \leq \frac{f(y)}{M\cdot g(y)}$:
        
a. If this holds, accept $y$ as a sample draw from $f$.
        
b. If not, reject the value of $y$ and return to the sampling step.

The algorithm will take an average of $M$ iterations to obtain a sample. The main problem with this process is that $M$ is generally large in high-dimensional spaces and since $f(accept) ∝ 1$, many samples will get rejected.


## 2. Importance Sampling

Our goal is to compute $E(f) =  \int f(x)p(x)dx$. 

If we have a density $q(x)$ which is easy to sample from, we can sample $x^{(i)} \sim q(x)$. Define the importance weight:

$$w(x^{(i)}) = \frac{p(x^{(i)})}{q(x^{(i)})}$$
Consider the weighted Monte Carlo sum:

$$\frac{1}{N} \sum_{i=1}^{N} f(x^{(i)})w(x^{(i)}) = \int f(x)g(x)dx$$


# 3. MCMC Method
Metropolis–Hastings and other MCMC algorithms are generally used for sampling from multi-dimensional distributions, especially when the number of dimensions is high. 

## 1 M-H Sampling
In Metropolis-Hastings sampling, samples mostly move towards higher density regions, but sometimes also move downhill. In comparison to rejection sampling where we always throw away the rejected samples, here we sometimes keep those samples as well.

  1. Init $x^{(0)}$

  2. for $i$ = 0 to $N-1$ do:
  
      $u \sim U(0,1)$ 
      
      $x^{\star} \sim q(x^{\star}|x^{(i)})$ 
      
      if $u \leq \min \{1,\frac{p(x^{\star}) q(x^{(i)}|x^{(\star)})}{p(x^{(i)}) q(x^{\star}|x^{(i)})} \}$ then,
      
      
        $x^{(i+1)} \leftarrow x^{\star}$
          
      else
      
        $x^{(i+1)} \rightarrow x^{(i)}$
        
      end if; end for
          
          
      
      
## 2 Gibbs Sampling


# 4. Other Methods

## 1. Slice Sampling

## 2. Hamiltonian Monte Carlo

## 3. Nested Sampling

## 4. Negative Sampling



