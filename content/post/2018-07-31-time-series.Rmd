---
title: Linear Time Series
author: Yuantong Li
date: '2018-08-01'
slug: time-series
categories:
  - Time Series
  - R
tags:
  - Time Series
  - R
description: ''
topics: []
---
# 1. Introduction to Time Series

Random Walk with drift:

$$X_t = \delta + X_{t-1} + W_{t}$$

```{r}
library(astsa, quietly = TRUE)
x = ts(cumsum(rnorm(500) + 0.2))
plot(x)
```

_Autocovariance function_:

$$\gamma_{X}(s,t) = E[(X_s - \mu_{X,s})(X_t - \mu_{X,t})]$$
and we know $\gamma_{X}(s,t) = \gamma_{X}(t,s)$

_Autocorrelation function (ACF)_:

$$\rho_{X}(s,t) = \frac{\gamma_{X}(s,t)}{\sqrt{\gamma_{X}(s,s)}\sqrt{\gamma_{X}(t,t)}}$$

_Cross correlation function (CCF)_:

$$\rho_{X,Y}(s,t) = \frac{\gamma_{X,Y}(s,t)}{\sqrt{\gamma_{X}(s,s)}\sqrt{\gamma_{Y}(t,t)}}$$

1. Strong (or strict) stationarity

A time series $X_t$ is called strongly stationary if the joint distribution of every finite collection of variables remain the same under time shifts, i.e.,

the joint distribution of ${X_{t_1} , . . . , X_{t_k}}$ is the same as that of ${X_{t_1+h},...,X_{t_k+h}}$ for all $t_1,...,t_k\in T$, for all $h$(positive or negative) and for all $k\geq 1$.

Strong stationarity is hard to verify.

Thus, for $k = 1$, every $X_t$ has the same distribution

2. Weak stationarity

A time series ${X_t}$ is weakly stationary if:

(1) the mean function $\mu_{X}(t)$ is constant; that is, every $X_t$ has the same mean;

(2) the autocovariance function $\gamma_{X}(s;t)$ depends on $s$ and $t$ only through their difference $|s-t|$.

Strongly stationary (plus finite variance) $\rightarrow$ weakly stationary.

In general, weak stationarity does not imply strong stationarity.

If $X_t$ is weakly stationary, then the covariance between $X_{t+h}$ and $X_t$ depends on $h$ but not on $t$, so that we may write the autocovariances as 

$$\gamma_{X}(h) = Cov(X_{t+h},X_{t}), for \hspace{1mm} all \hspace{1mm} t,h$$
$$\rho_{X}{h} = \frac{\gamma_{X}(h)}{\gamma_{X}(0)}$$

We can estimate the autocovariance function $\gamma(h)$ by

$$\hat{\gamma}(h) = n^{-1}\sum_{t = 1}^{n-h}(X_{t+h}-\bar{X}_n)(X_{t} - \bar{X}_n)$$

we can estimate the autocorrelation function (ACF) $\rho(h)$ by

$$\hat{\rho}(h) = \frac{\hat{\gamma}(h)}{\hat{\gamma}(0)}$$

If $X_t$ is white noise and $n$ is large and some mild conditions hold, then for each fixed $h$, $\hat{\rho}(h)$ is approximately normal with zero mean and standard deviation

$$\sigma_{\hat{\rho}(h)} = \frac{1}{\sqrt{n}}$$

So we can look for autocorrelations outside $\pm 2/\sqrt{n}$ as evidence of non-zero autocorrelation.

```{r}
acf(ts(rnorm(100)))
soi = scan("http://anson.ucdavis.edu/~shumway/soi.dat")
soi = ts(soi, start = 1950, frequency = 12)
recruit = scan("http://anson.ucdavis.edu/~shumway/recruit.dat")
recruit = ts(recruit, start = 1950, frequency = 12)
acf(soi, 50)
acf(recruit, 50)
ccf(soi, recruit, 50)
# Negative lags indicate SOI leads recruitment.
```


Box and Jenkins popularized an approach to time series analysis based on

- Auto-Regressive

- Intergrated

- Moving Average

Leading to the ARIMA models.

# 2. Autoregressive models (AR)

Recursive model _AR(2)_

$$X_t =a_1X_{t-1} +a_2X_{t-2} +W_t;t \geq 1; W_t\leftarrow N(0,1), iid$$
```{r}
w = ts(rnorm(500))
v = filter(w, filter = c(1, -0.9), method = "recursive")
plot(v)
```

For non-constant means, $EX_t = \mu_t, t \in T,$ we use

$$(X_t - \mu_t) = \phi_1(X_{t-1} - \mu_{t-1}) +\phi_2(X_{t-2} - \mu_{t-2}) + ... +\phi_p(X_{t-p} - \mu_{t-p}) + W_{t} $$


1. Trends and Detrending

$X_t = \mu_t + Y_t$

Where the obeserved $X_t$ is the sum of a mean part $\mu_t$ and a zero mean process $Y_t$.


- The form of trend might be linear,
- higher degree polynomials,
- regression functions based some suitable classes of basis functions, or
- some other function suggested by theory.

Thus, the first step is to estimate $\mu_t$ using any of the above approaches!


1.1 Example: 20th Century Global Temperature

```{r}
g1900 = window(globtemp, start = 1900)
plot(g1900)
```

Try fitted linear regression model:

```{r}
lmg1900 = lm(g1900~time(g1900))
plot(ts(residuals(lmg1900), start = 1900));
```


1.2 Differencing

Still appear nonstationary after detrending, A possible approach is to consider differencing the residuals.

Sepcifically, if $Y_t = \sum_{j = 1}^{t}W_t$ for a white noise process $\{W_t\}$, then, differencing yields:

$\nabla Y_t =  Y_t - Y_{t-1} = W_t$

which is stationary (and is a white noise process). Note that $\nabla Y_t = (1-B)Y_t$, where $B$ is the backshift operator, defined by $BY_t = Y_{t-1}$.

```{r}
plot(diff(g1900))

#ACFs
acf(residuals(lmg1900), 50)
acf(diff(g1900), 50)
```

1.3 Transformation

Common transformations include the logarithm, power transformation, etc.


2. AR(p)

Thus, in the operator form, we can write the AR(p) model as:

$$\phi(B) = [1-\phi_1B - \phi_2B - ... - \phi_p B^p]$$

$\phi(B)X_t = W_t$

2.1  Simulation
```{r}
plot(arima.sim(model = list(ar = .2), 100))
plot(arima.sim(model = list(ar = .9), 100))
plot(arima.sim(model = list(ar = -.9), 100))

```

2.2 Generating AR(p)-process samples

```{r}
plot(arima.sim(n=200, list(c(3,0,0), ar=c(-0.2, -0.1, -0.8))))
```


# 3. Moving Average model (MA)

$\{X_t\}$ is called a Moving Average process of order $q$ (written as MA(q), in short) if

$$X_t = W_t + \theta_1 W_{t-1} + ... + \theta_q W_{t-q}$$

where $\theta_1, ..., \theta_q$ are real numbers with $\theta_q \neq 0$ and where
$W_t \sim WN(0, \sigma_w^2)$


$$X_t = \theta(B) W_t$$
where $\theta (z) = 1 + \sum_{j=1}^{q}\theta_jz^j$ is the associated MA-polynomial.


# 4. Autoregressive Moving Average Models

$\{X_t\}$ is called an Autoregressive Moving Average process of order $p, q$ ( written as ARMA(p, q), in short) if


$$X_t = \phi_1 X_{t-1} + ... + \phi_pX_{t-p} + W_t + \theta_1W_{t-1} + ... + \theta_q W_{t-q}$$
In the operator form:  $\phi(B)X_t = \theta(B)W_t$ where the AR polynomial  $\phi(z)$ and the MA-polynomial $\theta(z)$ are as before!

1. Parameter redundancy: if $\phi(z)$  and $\theta(z)$ have any common factors, they can be cancelled out, so the model is the same as one with lower orders. We will assume no redundancy.

2. Causality: If $\phi(z) \neq 0$ for all $|z| \leq 1$, then $X_t$ can be written in terms of the present and the past $W_s$'s. We will assume causality.

3. Invertibility: If $\theta(z) \neq 0$ for all $|z| \leq 1$,  then $W_t$  can be written in terms of the present and the past $X_s$â€™s, and $X_t$ can be written as an infinite order autoregression. We assume invertibility.


## Yule-Walker equations
For a ARMA(p,q) process $\{Xt\}$, the recursive equation is

$$\gamma(h) = \sum_{j=1}^{p} \phi_j \gamma(h-j), h>q$$

## The partial autocorrelation function (PACF) fills that role.

Recall that for random variables $X$ and $Y$ and a random vector $Z$ (of an arbitrary dimension). The partial correlation of $X$ and $Y$ given $Z$ is the correlations of:

1. the residuals of X from its regression on Z, 
2. and the residuals of Y from its regression on Z.











