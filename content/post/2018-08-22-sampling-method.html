---
title: Sampling Methods and Monte Carlo methods
author: Yuantong
date: '2018-08-22'
slug: sampling-method
categories:
  - Bayesian
  - Computing
  - MCMC
tags:
  - Bayesian
  - Computing
  - MCMC
description: ''
topics: []
---



<p>Suppose we have a function <span class="math inline">\(f(x)\)</span> and we’d like to compute <span class="math inline">\(E{f(X )} =\int f(x)p(x)dx\)</span>, where <span class="math inline">\(g(x)\)</span> is a density.</p>
<p>There is no guarantee that the techniques we learn in calculus are sufficient to evaluate this integral analytically.</p>
<p>Thankfully, the law of large numbers (LLN) is here to help:</p>
<p>If X1, X2, . . . are iid samples from <span class="math inline">\(g(x)\)</span>, then</p>
<p><span class="math inline">\(\frac{1}{n}\sum^{n}_{i=1}f(x_{i}) \rightarrow \int f(x)p(x)dx = E[f(x)]\)</span> with prob 1</p>
<p>Suggests that a generic approximation of the integral be obtained by sampling lots of Xi ’s from <span class="math inline">\(g(x)\)</span> and replacing integration with averaging.</p>
<p>This is the heart of the Monte Carlo method.</p>
<div id="naive-method" class="section level1">
<h1>1. Naive method</h1>
<div id="inverse-of-cdf" class="section level2">
<h2>1. Inverse of CDF</h2>
<p>Inverse transform sampling is a method for generating random numbers from any probability distribution by using its inverse cumulative distribution <span class="math inline">\(F^{-1}(x)\)</span>. Recall that the cumulative distribution for a random variable X
is <span class="math inline">\(F_{X}(x)=P(X \leq x)\)</span>. A random variable <span class="math inline">\(U\)</span> uniformly distributed on <span class="math inline">\([0,1]\)</span>.</p>
<p>Step 1. Generate <span class="math inline">\(U \sim Unif(0,1)\)</span>.</p>
<p>Step 2. Let <span class="math inline">\(X = F^{-1}_{X}(U)\)</span>.</p>
<p>This method works when inverting <span class="math inline">\(F_{X}\)</span> is easy if <span class="math inline">\(X\)</span> is an exponential random variable, but its harder if <span class="math inline">\(X\)</span> is a Normal random variable. It works for discrete distributions.</p>
</div>
</div>
<div id="monte-carlo-method" class="section level1">
<h1>2. Monte Carlo method</h1>
<div id="accept---reject-sampling" class="section level2">
<h2>1. Accept - Reject Sampling</h2>
<p>The algorithm (used by John von Neumann and dating back to Buffon and his needle) to obtain a sample from distribution <span class="math inline">\(X\)</span> with density <span class="math inline">\(f\)</span> using samples from distribution <span class="math inline">\(Y\)</span> with density <span class="math inline">\(g\)</span> is as follows:</p>
<p>Step 1. Obtain a sample <span class="math inline">\(y\)</span> from distribution <span class="math inline">\(Y\)</span> and a sample<span class="math inline">\(u\)</span> from <span class="math inline">\(Unif(0,1)\)</span>.</p>
<p>Step 2. Check whether or not <span class="math inline">\(u \leq \frac{f(y)}{M\cdot g(y)}\)</span>:</p>
<ol style="list-style-type: lower-alpha">
<li><p>If this holds, accept <span class="math inline">\(y\)</span> as a sample draw from <span class="math inline">\(f\)</span>.</p></li>
<li><p>If not, reject the value of <span class="math inline">\(y\)</span> and return to the sampling step.</p></li>
</ol>
<p>The algorithm will take an average of <span class="math inline">\(M\)</span> iterations to obtain a sample. The main problem with this process is that <span class="math inline">\(M\)</span> is generally large in high-dimensional spaces and since <span class="math inline">\(f(accept) ∝ 1\)</span>, many samples will get rejected.</p>
</div>
<div id="importance-sampling" class="section level2">
<h2>2. Importance Sampling</h2>
<p>Our goal is to compute <span class="math inline">\(E(f) = \int f(x)p(x)dx\)</span>.</p>
<p>If we have a density <span class="math inline">\(q(x)\)</span> which is easy to sample from, we can sample <span class="math inline">\(x^{(i)} \sim q(x)\)</span>. Define the importance weight:</p>
<p><span class="math display">\[w(x^{(i)}) = \frac{p(x^{(i)})}{q(x^{(i)})}\]</span>
Consider the weighted Monte Carlo sum:</p>
<p><span class="math display">\[\frac{1}{N} \sum_{i=1}^{N} f(x^{(i)})w(x^{(i)}) = \int f(x)g(x)dx\]</span></p>
</div>
</div>
<div id="mcmc-method" class="section level1">
<h1>3. MCMC Method</h1>
<p>Metropolis–Hastings and other MCMC algorithms are generally used for sampling from multi-dimensional distributions, especially when the number of dimensions is high.</p>
<div id="m-h-sampling" class="section level2">
<h2>1 M-H Sampling</h2>
<p>In Metropolis-Hastings sampling, samples mostly move towards higher density regions, but sometimes also move downhill. In comparison to rejection sampling where we always throw away the rejected samples, here we sometimes keep those samples as well.</p>
<ol style="list-style-type: decimal">
<li><p>Init <span class="math inline">\(x^{(0)}\)</span></p></li>
<li><p>for <span class="math inline">\(i\)</span> = 0 to <span class="math inline">\(N-1\)</span> do:</p>
<p><span class="math inline">\(u \sim U(0,1)\)</span></p>
<p><span class="math inline">\(x^{\star} \sim q(x^{\star}|x^{(i)})\)</span></p>
<p>if <span class="math inline">\(u \leq \min \{1,\frac{p(x^{\star}) q(x^{(i)}|x^{(\star)})}{p(x^{(i)}) q(x^{\star}|x^{(i)})} \}\)</span> then,</p>
<p><span class="math inline">\(x^{(i+1)} \leftarrow x^{\star}\)</span></p>
<p>else</p>
<p><span class="math inline">\(x^{(i+1)} \rightarrow x^{(i)}\)</span></p>
<p>end if; end for</p></li>
</ol>
</div>
<div id="gibbs-sampling" class="section level2">
<h2>2 Gibbs Sampling</h2>
<p>The Gibbs sampler is specially adapted for multidimenional target distribution. The goal is to construct a Markov chain whose stationary distribution - or some marginalization thereof - equals the target distribution. The Gibbs sampler does this by sequentially sampling from univariate conditional distributions, which are often available in closed form.</p>
<p>Gibbs sampler in each dimension satisfies the detailed balance equalization, so the acceptance rate is 1. So Gibbs sampling method is part of M-H sampling method. (For the ith dimenison, the acceptance rate is 1, the other dimension acceptance rate is 0.)</p>
<ol style="list-style-type: decimal">
<li><p><em>Basic method</em>: specific ordering and random scan Gibbs sampling.</p></li>
<li><p><em>Blocking</em>: Blocking is typically useful when element of X are correlated, with the algorithm constructed so that more correlated elements are sampled together in one block.</p></li>
<li><p><em>Hybrid Gibbs Sampling</em>: Metropolis-within-Gibbs</p></li>
<li><p><em>Implemention</em>: Ensuring good mixing and convergence: simple graphical diagnostics, burn-in and run length, choice of propsal, reparameterization, comparing chains (effective sample size), number of chains</p></li>
</ol>
</div>
<div id="reversible-jump-mcmc" class="section level2">
<h2>3. Reversible Jump MCMC</h2>
<p>An MCMC algorithm that allows for model selection.</p>
<p><em>Examples: choosing number of clusters K, or even switching between two completely different models <span class="math inline">\(P_{1}(x)\)</span> and <span class="math inline">\(P_{2}(x)\)</span></em></p>
<p><a href="https://www.cs.cmu.edu/~epxing/Class/10708-14/lectures/lecture18-AdvMCMC.pdf">CMU Dr. Eric Xing Lecture Notes (18)</a></p>
</div>
</div>
<div id="other-methods" class="section level1">
<h1>4. Other Methods</h1>
<p>Advanced MCMC algorithms rely on auxiliary variables.</p>
<ol style="list-style-type: decimal">
<li><p><em><span class="math inline">\(P(x|v)\)</span> and <span class="math inline">\(P(v|x)\)</span> have simple forms.</em></p></li>
<li><p><em><span class="math inline">\(P(x,v)\)</span> is easy to navigate.</em></p></li>
</ol>
<div id="slice-sampling" class="section level2">
<h2>1. Slice Sampling</h2>
<p>Slice sampling is an auxiliary variable MCMC algorithm.</p>
<p><a href="https://www.cs.cmu.edu/~epxing/Class/10708-14/lectures/lecture18-AdvMCMC.pdf">CMU Dr. Eric Xing Lecture Notes (18)</a></p>
</div>
<div id="hamiltonian-monte-carlo" class="section level2">
<h2>2. Hamiltonian Monte Carlo</h2>
</div>
<div id="nested-sampling" class="section level2">
<h2>3. Nested Sampling</h2>
</div>
<div id="negative-sampling" class="section level2">
<h2>4. Negative Sampling</h2>
</div>
</div>
<div id="reference" class="section level1">
<h1>Reference:</h1>
<p><a href="http://www.mit.edu/~ilkery/papers/MetropolisHastingsSampling.pdf">Bayesian Inference: Metropolis-Hastings Sampling, Ilker Yildirim</a></p>
</div>
