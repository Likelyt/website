---
title: Conjugated gradient
author: Yuantong
date: '2019-07-20'
slug: conjugated-gradient
categories:
  - Optimization
tags:
  - Optimization
description: ''
topics: []
---
Two vectors $u$ and $v$ are conjugate with respect to the matrix $A$ if $u^{\prime} A v = 0$.

Suppose we want to minimize the quadratic function
$$
f(x)=\frac{1}{2} x^{\prime} A x-x^{\prime} b
$$
where $x$ is a $p$-dimensional vector and $A$ is a $p\times p$ symmetric matrix. $p_{0}=-f^{\prime}\left(x_{0}\right)=b-A x_{0}$, which is the negative gradient. So we can begin with the steepest descent direction $-f^{\prime}\left(x_{1}\right)$ but then we must modify it to make it conjugate to $p_{0}$. The constraint $p_{0}^{\prime} A p_{1}=0$ allows us to back calculate the next direction, starting with $-f^{\prime}\left(x_{1}\right)$, because we have

$$
p_{0}^{\prime} A\left(-f^{\prime}-\frac{p_{0}^{\prime} A\left(-f^{\prime}\right)}{p_{0}^{\prime} A p_{0}} p_{0}\right)=0
$$

Each time taking the steepest descent direction and modifying it to make it conjugate with the previous direction.