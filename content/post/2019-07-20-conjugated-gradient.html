---
title: Conjugated gradient
author: Yuantong
date: '2019-07-20'
slug: conjugated-gradient
categories:
  - Optimization
tags:
  - Optimization
description: ''
topics: []
---



<p>Two vectors <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span> are conjugate with respect to the matrix <span class="math inline">\(A\)</span> if <span class="math inline">\(u^{\prime} A v = 0\)</span>.</p>
<p>Suppose we want to minimize the quadratic function
<span class="math display">\[
f(x)=\frac{1}{2} x^{\prime} A x-x^{\prime} b
\]</span>
where <span class="math inline">\(x\)</span> is a <span class="math inline">\(p\)</span>-dimensional vector and <span class="math inline">\(A\)</span> is a <span class="math inline">\(p\times p\)</span> symmetric matrix. <span class="math inline">\(p_{0}=-f^{\prime}\left(x_{0}\right)=b-A x_{0}\)</span>, which is the negative gradient. So we can begin with the steepest descent direction <span class="math inline">\(-f^{\prime}\left(x_{1}\right)\)</span> but then we must modify it to make it conjugate to <span class="math inline">\(p_{0}\)</span>. The constraint <span class="math inline">\(p_{0}^{\prime} A p_{1}=0\)</span> allows us to back calculate the next direction, starting with <span class="math inline">\(-f^{\prime}\left(x_{1}\right)\)</span>, because we have</p>
<p><span class="math display">\[
p_{0}^{\prime} A\left(-f^{\prime}-\frac{p_{0}^{\prime} A\left(-f^{\prime}\right)}{p_{0}^{\prime} A p_{0}} p_{0}\right)=0
\]</span></p>
<p>Each time taking the steepest descent direction and modifying it to make it conjugate with the previous direction.</p>
